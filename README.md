# ğŸ­ Facial Emotion Recognition (FER)

A deep learning project that detects **human emotions** (Happy, Sad, Angry, Fear, Surprise, Disgust, Neutral) from facial expressions using **Convolutional Neural Networks (CNNs)**.  
Built with **TensorFlow/Keras** and **OpenCV** for real-time emotion recognition from webcam/video feed.

---

## âœ¨ Features
- ğŸ§  CNN-based classifier trained on **FER-2013 dataset**  
- ğŸ¥ Real-time emotion detection via **webcam** using OpenCV  
- ğŸ”„ Data augmentation for improved accuracy  
- ğŸ›¡ï¸ Dropout + Batch Normalization for reduced overfitting  
- ğŸŒ Deployable Flask web app for interactive demos  

---

## ğŸ“‚ Project Structure
```
ğŸ“ facial-emotion-recognition
 â”£ ğŸ“‚ models/             # Saved models
 â”£ ğŸ“‚ static/             # CSS/JS (for Flask app)
 â”£ ğŸ“‚ templates/          # HTML templates (for Flask app)
 â”£ ğŸ“œ train.py            # Training script
 â”£ ğŸ“œ evaluate.py         # Evaluation script
 â”£ ğŸ“œ app.py              # Flask app for deployment
 â”£ ğŸ“œ realtime.py         # Real-time webcam emotion detection
 â”£ ğŸ“œ requirements.txt    # Dependencies
 â”— ğŸ“œ README.md           # Project docs
```

---

## ğŸš€ Installation & Setup

### 1ï¸âƒ£ Clone the repo
```bash
git clone https://github.com/YOUR-USERNAME/facial-emotion-recognition.git
cd facial-emotion-recognition
```

### 2ï¸âƒ£ Create a virtual environment (recommended)
```bash
python3 -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run training (optional, pretrained model provided)
```bash
python train.py
```

### 5ï¸âƒ£ Run real-time recognition
```bash
python realtime.py
```

### 6ï¸âƒ£ Run Flask app
```bash
python app.py
```
Open [http://localhost:5000](http://localhost:5000) in your browser.

---

## ğŸ¥ Demo
### Real-time Emotion Detection  
![Demo GIF](assets/demo.gif)  
*(Add your screen recording here â€” recruiters love visuals!)*  

---

## ğŸ§ª Model Performance
- **Training Accuracy**: ~97%  
- **Validation Accuracy**: ~65% (after augmentation + regularization)  
- Evaluated on **FER-2013 dataset**  

---

## ğŸ“Š Sample Results
| Image | Predicted Emotion |
|-------|------------------|
| ![](assets/happy.png)   | ğŸ˜€ Happy |
| ![](assets/sad.png)     | ğŸ˜¢ Sad |
| ![](assets/angry.png)   | ğŸ˜  Angry |
| ![](assets/fear.png)    | ğŸ˜¨ Fear |
| ![](assets/surprise.png)| ğŸ˜² Surprise |
| ![](assets/disgust.png) | ğŸ¤¢ Disgust |
| ![](assets/neutral.png) | ğŸ˜ Neutral |


---

## ğŸ”® Future Improvements
- ğŸ”¹ Use **Transfer Learning** (ResNet, MobileNetV2) for higher accuracy  
- ğŸ”¹ Multi-modal recognition (facial + voice emotion analysis)  
- ğŸ”¹ Deploy on **Streamlit / Gradio / HuggingFace Spaces**  

---

## ğŸ§‘â€ğŸ’» Tech Stack
- **Python**  
- **TensorFlow/Keras**  
- **OpenCV**  
- **Flask**  

---

## ğŸ¤ Contributing
Pull requests are welcome! For major changes, please open an issue first to discuss what youâ€™d like to change.  

---

## ğŸ“œ License
This project is licensed under the **MIT License**.  

---

## â­ Support
If you like this project, consider giving it a **star â­** on GitHub â€” it helps others find it and motivates me to keep building cool projects!
